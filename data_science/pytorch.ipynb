{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2560b077-852d-48f3-812b-314b205f0caa",
   "metadata": {},
   "source": [
    "# Pytorch Basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "78d17e0e-fa91-420f-938a-e8516fbe4506",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "38ff1656-cdab-4d48-8361-b8e891c79df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([1, 2, 3], dtype=float)\n",
    "y = np.array([[5, 2, 3], [1, 3, 1]], dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b17b2c24-ad09-4ab6-a564-b6e6cc985901",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[6., 4., 6.],\n",
       "       [2., 5., 4.]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# broadcasts x from (1x3) -> (2x3) to add elementwise\n",
    "y + x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "145821d5-fdb8-48f8-89dc-b1707590bcfb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5., 4., 9.],\n",
       "       [1., 6., 3.]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# broadcasts x from (1x3) -> (2x3) to multiply elementwise\n",
    "y * x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "63eccc17-640f-418b-ad62-6c146f87a6d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([18., 10.])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate the dot product\n",
    "y @ x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "be8cc47c-74f6-4197-bd5b-8b7fe2a2792b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x = tensor([1., 2., 3.], dtype=torch.float64)\n",
      "y = tensor([[5., 2., 3.],\n",
      "        [1., 3., 1.]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "# change to tensors\n",
    "x = torch.tensor(x)\n",
    "y = torch.tensor(y)\n",
    "# change to numpy\n",
    "x = x.numpy()\n",
    "y = y.numpy()\n",
    "# change to tensors\n",
    "x = torch.tensor(x)\n",
    "y = torch.tensor(y)\n",
    "print(f'{x = }')\n",
    "print(f'{y = }')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1078396-fc34-4bbe-82cb-4e999a43cf24",
   "metadata": {},
   "source": [
    "## Network from Scratch with Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5ccabeaa-f761-4b05-99ae-ba29c8c865ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define functions for layer 1 and 2\n",
    "#  hidden layer = f\n",
    "#  output layer = g\n",
    "def f(x):\n",
    "    return x[0]**2 + 2 * (x[1]**2)\n",
    "def g(x):\n",
    "    return (3*(x**2) - 2*x + 10).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2a59f38b-c65b-4a94-933c-249d83d41712",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/amdgpu/share/libdrm/amdgpu.ids: No such file or directory\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(2354.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the flow of input to output is: \n",
    "# x = input, v = output\n",
    "#  x -> f -> u\n",
    "#  u -> g -> v\n",
    "x = torch.tensor([[1, 2, 3, 4], [-1, -2, -3, -4]], \n",
    "                 dtype=float, \n",
    "                 requires_grad=True)\n",
    "u = f(x)\n",
    "v = g(u)\n",
    "v.backward() # backpropagate: update gradients\n",
    "v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1c026d13-310a-47ab-ba11-2cc71e7d4e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use this to perform operation without affecting stored gradients\n",
    "with torch.no_grad():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f7c4d777-eb0b-4315-9cdf-c2dbee035562",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2354.5000, dtype=torch.float64)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use .detach() to:\n",
    "#   remove the gradient (grad_fn=.. not shown)\n",
    "#   remove the node from the network\n",
    "v.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cabb7c6f-506c-40b0-ae59-87f66fcb3405",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset tensor x\n",
    "x = torch.tensor([[1, 2, 3, 4], [-1, -2, -3, -4]], \n",
    "                 dtype=float, \n",
    "                 requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c9d9ac1d-018f-429a-9dd6-e8276b216f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stochastic gradient descent\n",
    "optimizer = torch.optim.SGD([x], lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5c20bafc-76bf-4836-98f9-63ff6baafb6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch0_loss: 2354.5\n",
      "epoch1_loss: 9680.58271084\n",
      "epoch2_loss: 10495288.069958264\n",
      "epoch3_loss: 2.598978266316877e+16\n",
      "epoch4_loss: 4.03786934268059e+44\n",
      "epoch5_loss: 1.51619361030149e+129\n",
      "epoch6_loss: inf\n",
      "epoch7_loss: nan\n",
      "epoch8_loss: nan\n",
      "epoch9_loss: nan\n"
     ]
    }
   ],
   "source": [
    "# try to train for 10 epochs\n",
    "# the loss/gradient will explode before 10 epochs\n",
    "for i in range(10):\n",
    "    optimizer.zero_grad() # clears previous gradients\n",
    "    loss = g(f(x)) # computes the loss\n",
    "    loss.backward() # computes the gradients\n",
    "    optimizer.step() # steps to greatest descent\n",
    "    print(f'epoch{i}_loss: {loss.detach()}') # detaches to save memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7926a30-f55e-473f-8064-948531f3dacb",
   "metadata": {},
   "source": [
    "## Gradient Descent with Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6c493221-6874-4952-af86-377ddd4bf94e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define new a new function that wont explode f(x)\n",
    "def f(x):\n",
    "    return x[0]**2 + 2 * x[1]**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9083ec19-4e6f-4139-9cc0-7cda9120c79c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use 1 equation and x = [-5, -2] for better convergence example\n",
    "x = torch.tensor([-5, -2], dtype=float, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eca4da4c-dbb5-40ec-8771-6767d6722790",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD([x], lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7440f795-d545-4e4d-bc69-6c3b7d71ef21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch0_loss: 33.0\n",
      "epoch1_loss: 31.382800000000003\n",
      "epoch2_loss: 29.853976480000004\n",
      "epoch3_loss: 28.408121839168\n",
      "epoch4_loss: 27.040192196415312\n",
      "epoch5_loss: 25.745481260120684\n",
      "epoch6_loss: 24.51959615200815\n",
      "epoch7_loss: 23.358435035713462\n",
      "epoch8_loss: 22.258166412144174\n",
      "epoch9_loss: 21.215209954406795\n",
      "epoch10_loss: 20.226218764912623\n",
      "epoch11_loss: 19.288062946345917\n",
      "epoch12_loss: 18.397814386543782\n",
      "epoch13_loss: 17.552732665052556\n",
      "epoch14_loss: 16.750251996240255\n",
      "epoch15_loss: 15.987969130406578\n",
      "epoch16_loss: 15.263632140383667\n",
      "epoch17_loss: 14.575130026702434\n",
      "epoch18_loss: 13.920483079547264\n",
      "epoch19_loss: 13.297833940470303\n"
     ]
    }
   ],
   "source": [
    "for i in range(20):\n",
    "    optimizer.zero_grad() \n",
    "    loss = f(x)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f'epoch{i}_loss: {loss.detach()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a975b9a7-9c3f-4e49-9e19-cb8fcdd9a20b",
   "metadata": {},
   "source": [
    "## Gradient Descent without Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "604bbd10-78bf-4805-a0f6-fd987cc53d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([-5, -2], dtype=float, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "03db769d-dd1f-4825-ac85-85395dfdb7b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch0_loss: 33.0\n",
      "epoch1_loss: 31.382800000000003\n",
      "epoch2_loss: 29.853976480000004\n",
      "epoch3_loss: 28.408121839168\n",
      "epoch4_loss: 27.040192196415312\n",
      "epoch5_loss: 25.745481260120684\n",
      "epoch6_loss: 24.51959615200815\n",
      "epoch7_loss: 23.358435035713462\n",
      "epoch8_loss: 22.258166412144174\n",
      "epoch9_loss: 21.215209954406795\n",
      "epoch10_loss: 20.226218764912623\n",
      "epoch11_loss: 19.288062946345917\n",
      "epoch12_loss: 18.397814386543782\n",
      "epoch13_loss: 17.552732665052556\n",
      "epoch14_loss: 16.750251996240255\n",
      "epoch15_loss: 15.987969130406578\n",
      "epoch16_loss: 15.263632140383667\n",
      "epoch17_loss: 14.575130026702434\n",
      "epoch18_loss: 13.920483079547264\n",
      "epoch19_loss: 13.297833940470303\n"
     ]
    }
   ],
   "source": [
    "lr = 0.01\n",
    "for i in range(20):\n",
    "    loss = f(x)\n",
    "    loss.backward()\n",
    "    with torch.no_grad():\n",
    "        x -= lr * x.grad # update parameters\n",
    "        x.grad.zero_() # manually zero gradients after\n",
    "    print(f'epoch{i}_loss: {loss.detach()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfb77598-129a-4b44-8726-66e10500aa40",
   "metadata": {},
   "source": [
    "## Gradient Descent from Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "afe70958-28e6-4606-be10-9a6ad59829e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# g(x) is the gradient of f(x)\n",
    "def f(x):\n",
    "    return x[0]**2 + 2 * x[1]**2\n",
    "def g(x):\n",
    "    return np.array([2*x[0], 4*x[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "63a536c5-bacd-4059-9714-5af17cee5872",
   "metadata": {},
   "outputs": [],
   "source": [
    "# implement gradient descent from scratch without the optimizer\n",
    "def gradient_descent(x0, step, num_steps):\n",
    "    x = np.array(x0)\n",
    "    history = [[x[0], x[1], f(x)]]\n",
    "    for i in range(num_steps):\n",
    "        gradient = g(x)\n",
    "        dx = step * gradient\n",
    "        x = x - dx\n",
    "        history.append([x[0], x[1], f(x)])\n",
    "    return np.array(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a996c08a-944d-4500-84b2-9d36f9719417",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = gradient_descent((-5, -2), step=0.01, num_steps=19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "df924f60-dcc4-47f6-b7d1-65f935dd8f08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch0_loss: 33.0\n",
      "epoch1_loss: 31.382800000000003\n",
      "epoch2_loss: 29.853976480000004\n",
      "epoch3_loss: 28.408121839168\n",
      "epoch4_loss: 27.040192196415312\n",
      "epoch5_loss: 25.745481260120684\n",
      "epoch6_loss: 24.51959615200815\n",
      "epoch7_loss: 23.358435035713462\n",
      "epoch8_loss: 22.258166412144174\n",
      "epoch9_loss: 21.215209954406795\n",
      "epoch10_loss: 20.226218764912623\n",
      "epoch11_loss: 19.288062946345917\n",
      "epoch12_loss: 18.397814386543782\n",
      "epoch13_loss: 17.552732665052556\n",
      "epoch14_loss: 16.750251996240255\n",
      "epoch15_loss: 15.987969130406578\n",
      "epoch16_loss: 15.263632140383667\n",
      "epoch17_loss: 14.575130026702434\n",
      "epoch18_loss: 13.920483079547264\n",
      "epoch19_loss: 13.297833940470303\n"
     ]
    }
   ],
   "source": [
    "epoch = 0\n",
    "for x0, x1, y in history:\n",
    "    print(f'epoch{epoch}_loss: {y}')\n",
    "    epoch += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c36ee1c9-99b7-4e76-b510-4645b5a4c3b1",
   "metadata": {},
   "source": [
    "## Stochastic Gradient Descent from Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a5557958-12c5-4fa2-b4f3-1a95177df3ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# implement stochastic gradient descent from scratch without the optimizer\n",
    "# this just simulates how it works. actual SGD performs better\n",
    "def stochastic_gradient_descent(x0, step, num_steps):\n",
    "    x = np.array(x0)\n",
    "    history = [[x[0], x[1], f(x)]]\n",
    "    for i in range(num_steps):\n",
    "        true_gradient = g(x)\n",
    "        # simulates the natural noise of data \n",
    "        noise = np.random.normal(0, 1.5, size=x.shape) # mean=0, std=1.5\n",
    "        stochastic_gradient = true_gradient + noise\n",
    "        dx = step * stochastic_gradient\n",
    "        x = x - dx\n",
    "        history.append([x[0], x[1], f(x)])\n",
    "    return np.array(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a70a2d5d-bc53-455f-91f9-ddfe9d899d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = stochastic_gradient_descent((-5, -2), step=0.01, num_steps=19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "43426336-c14f-4f4b-b2c9-cbaffeee9470",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch1_loss: 33.0\n",
      "epoch2_loss: 31.3492616527654\n",
      "epoch3_loss: 30.042182826756978\n",
      "epoch4_loss: 28.568651324520665\n",
      "epoch5_loss: 27.099011892757616\n",
      "epoch6_loss: 25.84270129967055\n",
      "epoch7_loss: 24.31611137409537\n",
      "epoch8_loss: 23.198138186529228\n",
      "epoch9_loss: 22.148425752499683\n",
      "epoch10_loss: 21.323934753288103\n",
      "epoch11_loss: 20.32407157563845\n",
      "epoch12_loss: 19.730218268569427\n",
      "epoch13_loss: 18.619518178393186\n",
      "epoch14_loss: 17.510458908358988\n",
      "epoch15_loss: 16.827931920949144\n",
      "epoch16_loss: 16.20585525000218\n",
      "epoch17_loss: 15.404857327690376\n",
      "epoch18_loss: 14.709496834174645\n",
      "epoch19_loss: 14.100490724399016\n",
      "epoch20_loss: 13.510629715747092\n"
     ]
    }
   ],
   "source": [
    "epoch = 0\n",
    "for x0, x1, y in history:\n",
    "    epoch += 1\n",
    "    print(f'epoch{epoch}_loss: {y}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb345c7-6fb0-4f7f-bf11-8a3b7351fc25",
   "metadata": {},
   "source": [
    "## Momentum Gradient Descent from Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e6cba8c4-cc04-495c-a93b-f4012f0e2300",
   "metadata": {},
   "outputs": [],
   "source": [
    "# implement momentum gradient descent from scratch without the optimizer\n",
    "# beta is the percentage of velocity to retain from prev step\n",
    "# x_new = x_old - velocity\n",
    "def momentum_gradient_descent(x0, step, num_steps, beta=0.8):\n",
    "    x = np.array(x0)\n",
    "    dx = np.zeros_like(x)\n",
    "    history = [[x[0], x[1], f(x)]]\n",
    "    for i in range(num_steps):\n",
    "        gradient = g(x)\n",
    "        dx = step * gradient + beta * dx\n",
    "        x = x - dx\n",
    "        history.append([x[0], x[1], f(x)])\n",
    "    return np.array(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c88f5b4d-fa96-475c-83af-a31eff47fce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = momentum_gradient_descent((-5, -2), step=0.01, num_steps=19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "76d0fc6d-ad0e-4fc0-96b0-26d6999a461f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch0_loss: 33.0\n",
      "epoch1_loss: 31.382800000000003\n",
      "epoch2_loss: 28.628389280000004\n",
      "epoch3_loss: 25.207211492928003\n",
      "epoch4_loss: 21.524154336581454\n",
      "epoch5_loss: 17.891827195148906\n",
      "epoch6_loss: 14.525311361163372\n",
      "epoch7_loss: 11.55092878825017\n",
      "epoch8_loss: 9.022658706056212\n",
      "epoch9_loss: 6.9412746602224225\n",
      "epoch10_loss: 5.27278611021891\n",
      "epoch11_loss: 3.9641439910793164\n",
      "epoch12_loss: 2.9552893679009293\n",
      "epoch13_loss: 2.1874417854391126\n",
      "epoch14_loss: 1.6080465783485112\n",
      "epoch15_loss: 1.173070351379103\n",
      "epoch16_loss: 0.8474103338387285\n",
      "epoch17_loss: 0.6041290709607978\n",
      "epoch18_loss: 0.4230976757475776\n",
      "epoch19_loss: 0.28947408430327937\n"
     ]
    }
   ],
   "source": [
    "epoch = 0\n",
    "for x0, x1, y in history:\n",
    "    print(f'epoch{epoch}_loss: {y}')\n",
    "    epoch += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68bd3cfe-5af8-4752-9a90-faf542edc3a8",
   "metadata": {},
   "source": [
    "## Adam Gradient Descent from Scrath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "be2859d9-ab87-433d-b8bf-768df0ab4da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# implement adam gradient descent from scratch without the optimizer\n",
    "# epsilon is a small values thats added to prevent /0 for friction\n",
    "# x_new = x_old - velocity / friction\n",
    "def adam_gradient_descent(x0, step, num_steps, beta1=0.8, beta2=0.999, epsilon=1e-8):\n",
    "    x = np.array(x0)\n",
    "    m = np.zeros_like(x) # momentum part\n",
    "    v = np.zeros_like(x) # RMSProp\n",
    "    history = [[x[0], x[1], f(x)]]\n",
    "    for t in range(1, num_steps + 1):\n",
    "        gradient = g(x)\n",
    "        m = beta1 * m + (1 - beta1) * gradient # update momentum: mean of gradients\n",
    "        v = beta2 * v + (1 - beta2) * (gradient**2) # update velocity: mean of gradients^2\n",
    "        m_hat = m / (1 - beta1**t)# boost small values m and v, on startup\n",
    "        v_hat = v / (1 - beta2**t)\n",
    "        x = x - step * m_hat / (np.sqrt(v_hat) + epsilon) # update parameters\n",
    "        history.append([x[0], x[1], f(x)])\n",
    "    return np.array(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7c12bc19-cf9b-48ac-85d7-6bcc90caf8bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = adam_gradient_descent((-5, -2), step=0.01, num_steps=19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9b7b6f55-1803-4a7f-804c-da90e5bbc40d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch0_loss: 33.0\n",
      "epoch1_loss: 32.8203000001993\n",
      "epoch2_loss: 32.641233282723995\n",
      "epoch3_loss: 32.46282135062469\n",
      "epoch4_loss: 32.28508494115806\n",
      "epoch5_loss: 32.10804383941745\n",
      "epoch6_loss: 31.931716718604264\n",
      "epoch7_loss: 31.756121010764296\n",
      "epoch8_loss: 31.58127281008595\n",
      "epoch9_loss: 31.40718680915527\n",
      "epoch10_loss: 31.233876267018715\n",
      "epoch11_loss: 31.061353006619456\n",
      "epoch12_loss: 30.88962743820752\n",
      "epoch13_loss: 30.7187086046996\n",
      "epoch14_loss: 30.548604244666247\n",
      "epoch15_loss: 30.37932086861347\n",
      "epoch16_loss: 30.21086384444574\n",
      "epoch17_loss: 30.04323748838567\n",
      "epoch18_loss: 29.876445158120006\n",
      "epoch19_loss: 29.71048934548749\n"
     ]
    }
   ],
   "source": [
    "# notice it looks worse. Adam takes consistent small steps regardless of slope\n",
    "# this is more stable for exploding gradients\n",
    "# in actual training itll perform better\n",
    "epoch = 0\n",
    "for x0, x1, y in history:\n",
    "    print(f'epoch{epoch}_loss: {y}')\n",
    "    epoch += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
